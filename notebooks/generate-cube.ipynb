{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from os.path import basename, splitext\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import shapefile\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "sf_path = \"../data/akl_points.shp\"\n",
    "data_dir=\"../data/results\"\n",
    "tz = timezone(\"Pacific/Auckland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapefile with 2011 points\n",
      "INCOMPLETE WARNING: Number of result files 6561 should equal 10055, the number of time indices '5' times number of points '2011'\n",
      "Found 6561 files representing 2011 points at 5 time slices\n",
      "Estimated cube size 77.1 MiB\n"
     ]
    }
   ],
   "source": [
    "# data loading and sanity checking\n",
    "\n",
    "# shapefile\n",
    "points = shapefile.Reader(sf_path)\n",
    "npt = len(points.records())\n",
    "print(f\"Loaded shapefile with {npt} points\")\n",
    "\n",
    "# odt spatial index\n",
    "geoids = sorted([r[0] for r in points.records()])\n",
    "loc_idx = {g:i for i, g in enumerate(geoids)}\n",
    "assert len(loc_idx) == npt, \"Shapefile may contain duplicate identifiers\"\n",
    "\n",
    "file_paths = glob(os.path.join(data_dir, \"*.csv\"))\n",
    "nf = len(file_paths)\n",
    "                                  \n",
    "# generate time index\n",
    "query_times = [splitext(basename(f))[0] for f in file_paths]\n",
    "query_times = [datetime(*map(int, t.split('_')[1:])) for t in query_times]\n",
    "query_times = sorted(set(query_times))\n",
    "t_idx = {t:i for i, t in enumerate(sorted(set(query_times)))}\n",
    "nt = len(t_idx)\n",
    "if nf != nt * npt:\n",
    "    print(f\"INCOMPLETE WARNING: Number of result files {nf} should equal {nt * npt}, the number of time indices '{nt}' times number of points '{npt}'\")\n",
    "\n",
    "print(f\"Found {nf} files representing {npt} points at {nt} time slices\")\n",
    "print(f\"Estimated cube size {nt * npt * npt * 4 / (1024 ** 2):.1f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result(file_path):\n",
    "           \n",
    "    # process filename\n",
    "    bn = os.path.basename(file_path)\n",
    "    tokens = os.path.splitext(bn)[0].split(\"_\")\n",
    "    geoid, Y, M, D, h, m, s = map(int, tokens)\n",
    "    dt = datetime(Y, M, D, h, m, s,)\n",
    "    \n",
    "    # read csv data intof dataframe\n",
    "    df = pd.read_csv(file_path, parse_dates=[\"queryTime\"], infer_datetime_format=True) \n",
    "    \n",
    "    return geoid, dt, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data 100.0 %\r"
     ]
    }
   ],
   "source": [
    "odt = np.ones((npt, npt, nt), dtype=np.float32) * np.nan\n",
    "for i, f in enumerate(file_paths):\n",
    "    geoid, dt, df = load_result(f)\n",
    "    o = loc_idx[geoid]\n",
    "    d = [loc_idx[p] for p in df.toPlace]\n",
    "    t = t_idx[dt]\n",
    "    odt[o, d, t] = df.eta     \n",
    "    print(f\"loading data {100 * (i+1)/nf:.1f} %\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save precomputed odt\n",
    "with open(\"../data/akl_odt.npy\", 'wb') as f:\n",
    "    np.save(f, odt)\n",
    "    \n",
    "# save location index\n",
    "with open(\"../data/akl_loc_idx.pkl\", 'wb') as f:\n",
    "    pickle.dump(loc_idx, f)\n",
    "\n",
    "# save time index\n",
    "with open(\"../data/akl_t_idx.pkl\", 'wb') as f:\n",
    "    pickle.dump(t_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(odt[:256, :256, 0]/np.nanmax(odt))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
